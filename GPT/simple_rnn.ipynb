{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597642344942",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers, losses, metrics\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n"
    }
   ],
   "source": [
    "# 캐릭터 글자 목록\n",
    "char_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "             'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# 캐릭터 사전 생성\n",
    "char_to_idx = {c: i for i, c in enumerate(char_list)}\n",
    "dic_len = len(char_to_idx)\n",
    "\n",
    "print(char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 목록\n",
    "# 앞의 세 글자가 주어지면 마지막 글자를 예측\n",
    "# lov -> e\n",
    "word_list = ['love', 'look', 'face', 'fast', 'home', 'hope',\n",
    "             'good', 'gold', 'tree', 'true', 'road', 'rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 데이터 생성\n",
    "def make_batch(word_list):\n",
    "    \n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for word in word_list:\n",
    "        # 입력 단어를 인덱스로 변환\n",
    "        input = [char_to_idx[c] for c in word[:-1]]\n",
    "        \n",
    "        # 목표 캐릭터를 인덱스로 변환\n",
    "        target = char_to_idx[word[-1]]\n",
    "\n",
    "        # 입력 인덱스를 원핫인코딩으로 변환\n",
    "        input_batch.append(np.eye(dic_len)[input])\n",
    "\n",
    "        # 목표 인덱스를 원핫인코딩으로 변환\n",
    "        target_batch.append(np.eye(dic_len)[target])\n",
    "\n",
    "    return np.array(input_batch), np.array(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력/목표 배치 생성\n",
    "x_train, y_train = make_batch(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "y_train[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 글자만큼 타임스텝 반복\n",
    "time_step = 3\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.LSTM(64, input_shape = (time_step, dic_len)))\n",
    "    model.add(layers.Dense(dic_len, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/100\n12/12 [==============================] - 1s 45ms/step - loss: 3.2309 - acc: 0.0833\nEpoch 2/100\n12/12 [==============================] - 0s 5ms/step - loss: 3.1384 - acc: 0.5000\nEpoch 3/100\n12/12 [==============================] - 0s 5ms/step - loss: 3.0405 - acc: 0.5833\nEpoch 4/100\n12/12 [==============================] - 0s 5ms/step - loss: 2.9101 - acc: 0.6667\nEpoch 5/100\n12/12 [==============================] - 0s 5ms/step - loss: 2.7252 - acc: 0.5000\nEpoch 6/100\n12/12 [==============================] - 0s 5ms/step - loss: 2.4670 - acc: 0.5000\nEpoch 7/100\n12/12 [==============================] - 0s 5ms/step - loss: 2.1316 - acc: 0.5000\nEpoch 8/100\n12/12 [==============================] - 0s 5ms/step - loss: 1.7681 - acc: 0.5000\nEpoch 9/100\n12/12 [==============================] - 0s 5ms/step - loss: 1.5018 - acc: 0.5000\nEpoch 10/100\n12/12 [==============================] - 0s 5ms/step - loss: 1.3474 - acc: 0.5000\nEpoch 11/100\n12/12 [==============================] - 0s 5ms/step - loss: 1.2646 - acc: 0.5000\nEpoch 12/100\n12/12 [==============================] - 0s 5ms/step - loss: 1.2137 - acc: 0.5000\nEpoch 13/100\n12/12 [==============================] - 0s 5ms/step - loss: 1.1744 - acc: 0.5000\nEpoch 14/100\n12/12 [==============================] - 0s 5ms/step - loss: 1.1454 - acc: 0.5000\nEpoch 15/100\n12/12 [==============================] - 0s 5ms/step - loss: 1.1201 - acc: 0.5000\nEpoch 16/100\n12/12 [==============================] - 0s 5ms/step - loss: 1.0914 - acc: 0.5000\nEpoch 17/100\n12/12 [==============================] - 0s 5ms/step - loss: 1.0664 - acc: 0.5000\nEpoch 18/100\n12/12 [==============================] - 0s 5ms/step - loss: 1.0431 - acc: 0.5000\nEpoch 19/100\n12/12 [==============================] - 0s 5ms/step - loss: 1.0178 - acc: 0.5000\nEpoch 20/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.9971 - acc: 0.5833\nEpoch 21/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.9684 - acc: 0.5833\nEpoch 22/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.9413 - acc: 0.6667\nEpoch 23/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.9156 - acc: 0.6667\nEpoch 24/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.8870 - acc: 0.6667\nEpoch 25/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.8625 - acc: 0.6667\nEpoch 26/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.8271 - acc: 0.6667\nEpoch 27/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.8030 - acc: 0.6667\nEpoch 28/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.7688 - acc: 0.6667\nEpoch 29/100\n12/12 [==============================] - 0s 6ms/step - loss: 0.7447 - acc: 0.6667\nEpoch 30/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.7114 - acc: 0.7500\nEpoch 31/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.6782 - acc: 0.7500\nEpoch 32/100\n12/12 [==============================] - 0s 6ms/step - loss: 0.6582 - acc: 0.7500\nEpoch 33/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.6228 - acc: 0.7500\nEpoch 34/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.5961 - acc: 0.7500\nEpoch 35/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.5655 - acc: 0.7500\nEpoch 36/100\n12/12 [==============================] - 0s 6ms/step - loss: 0.5403 - acc: 0.8333\nEpoch 37/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.5156 - acc: 0.8333\nEpoch 38/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.4895 - acc: 0.8333\nEpoch 39/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.4704 - acc: 0.8333\nEpoch 40/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.4494 - acc: 0.8333\nEpoch 41/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.4323 - acc: 0.8333\nEpoch 42/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.4116 - acc: 0.8333\nEpoch 43/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.3917 - acc: 0.8333\nEpoch 44/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.3753 - acc: 0.9167\nEpoch 45/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.3583 - acc: 0.9167\nEpoch 46/100\n12/12 [==============================] - 0s 6ms/step - loss: 0.3431 - acc: 1.0000\nEpoch 47/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.3320 - acc: 0.8333\nEpoch 48/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.3154 - acc: 1.0000\nEpoch 49/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.3053 - acc: 1.0000\nEpoch 50/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.2870 - acc: 1.0000\nEpoch 51/100\n12/12 [==============================] - 0s 6ms/step - loss: 0.2812 - acc: 1.0000\nEpoch 52/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.2722 - acc: 0.9167\nEpoch 53/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.2642 - acc: 1.0000\nEpoch 54/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.2492 - acc: 1.0000\nEpoch 55/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.2450 - acc: 1.0000\nEpoch 56/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.2348 - acc: 1.0000\nEpoch 57/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.2251 - acc: 1.0000\nEpoch 58/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.2190 - acc: 1.0000\nEpoch 59/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.2095 - acc: 1.0000\nEpoch 60/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.2030 - acc: 1.0000\nEpoch 61/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1930 - acc: 1.0000\nEpoch 62/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1906 - acc: 1.0000\nEpoch 63/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1846 - acc: 1.0000\nEpoch 64/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1772 - acc: 1.0000\nEpoch 65/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1670 - acc: 1.0000\nEpoch 66/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1682 - acc: 1.0000\nEpoch 67/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1571 - acc: 1.0000\nEpoch 68/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1498 - acc: 1.0000\nEpoch 69/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1479 - acc: 1.0000\nEpoch 70/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1438 - acc: 1.0000\nEpoch 71/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1346 - acc: 1.0000\nEpoch 72/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1322 - acc: 1.0000\nEpoch 73/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1270 - acc: 1.0000\nEpoch 74/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.1242 - acc: 1.0000\nEpoch 75/100\n12/12 [==============================] - 0s 6ms/step - loss: 0.1164 - acc: 1.0000\nEpoch 76/100\n12/12 [==============================] - 0s 6ms/step - loss: 0.1114 - acc: 1.0000\nEpoch 77/100\n12/12 [==============================] - 0s 6ms/step - loss: 0.1096 - acc: 1.0000\nEpoch 78/100\n12/12 [==============================] - 0s 6ms/step - loss: 0.1011 - acc: 1.0000\nEpoch 79/100\n12/12 [==============================] - 0s 6ms/step - loss: 0.1001 - acc: 1.0000\nEpoch 80/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0960 - acc: 1.0000\nEpoch 81/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0942 - acc: 1.0000\nEpoch 82/100\n12/12 [==============================] - 0s 6ms/step - loss: 0.0878 - acc: 1.0000\nEpoch 83/100\n12/12 [==============================] - 0s 6ms/step - loss: 0.0872 - acc: 1.0000\nEpoch 84/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0807 - acc: 1.0000\nEpoch 85/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0777 - acc: 1.0000\nEpoch 86/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0769 - acc: 1.0000\nEpoch 87/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0700 - acc: 1.0000\nEpoch 88/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0691 - acc: 1.0000\nEpoch 89/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0657 - acc: 1.0000\nEpoch 90/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0658 - acc: 1.0000\nEpoch 91/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0602 - acc: 1.0000\nEpoch 92/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0585 - acc: 1.0000\nEpoch 93/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0538 - acc: 1.0000\nEpoch 94/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0502 - acc: 1.0000\nEpoch 95/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0515 - acc: 1.0000\nEpoch 96/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0473 - acc: 1.0000\nEpoch 97/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0444 - acc: 1.0000\nEpoch 98/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0430 - acc: 1.0000\nEpoch 99/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0425 - acc: 1.0000\nEpoch 100/100\n12/12 [==============================] - 0s 5ms/step - loss: 0.0395 - acc: 1.0000\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x1e75e6b5a08>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = build_model()\n",
    "\n",
    "# 훈련 시작\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=100,\n",
    "          batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[5.50502222e-09, 4.74356243e-09, 4.04384082e-09, 4.96363675e-04,\n        9.49670970e-01, 5.97114491e-09, 3.87422716e-09, 8.41021475e-09,\n        4.13837942e-09, 5.08541254e-09, 4.87111919e-02, 3.25193361e-09,\n        5.18551557e-09, 6.54680932e-09, 2.25573027e-09, 4.60557015e-09,\n        7.62991537e-09, 2.73120948e-09, 5.29540101e-09, 1.12129853e-03,\n        6.39206732e-09, 6.18729112e-09, 2.92639002e-09, 4.72743977e-09,\n        4.23271462e-09, 7.18073778e-09],\n       [9.55346557e-09, 9.29253297e-09, 6.99690750e-09, 8.02447461e-03,\n        1.07272476e-01, 9.12251963e-09, 7.54979190e-09, 1.63752336e-08,\n        8.04629785e-09, 8.92133301e-09, 8.80242705e-01, 7.54676854e-09,\n        8.48177528e-09, 1.09040963e-08, 4.78201745e-09, 9.45930623e-09,\n        1.47046402e-08, 4.82903761e-09, 8.92318752e-09, 4.46020113e-03,\n        1.09384244e-08, 1.46884940e-08, 7.39404138e-09, 8.12654566e-09,\n        8.05030442e-09, 1.75797243e-08],\n       [4.73548845e-08, 3.78059823e-08, 2.49046277e-08, 1.20908982e-04,\n        9.70795631e-01, 4.78027289e-08, 2.45631728e-08, 6.24482652e-08,\n        4.04096312e-08, 3.30319914e-08, 4.56232857e-03, 2.69097900e-08,\n        3.21060263e-08, 4.69525894e-08, 1.62321072e-08, 2.31847270e-08,\n        5.62501157e-08, 1.88990175e-08, 3.41704975e-08, 2.45203711e-02,\n        3.77246572e-08, 3.48840139e-08, 2.09424602e-08, 3.18188320e-08,\n        2.44315483e-08, 6.68511788e-08],\n       [3.59001262e-07, 4.77255924e-07, 2.08912084e-07, 1.01220212e-03,\n        2.44797654e-02, 2.89820349e-07, 2.60049831e-07, 5.74783144e-07,\n        5.12926022e-07, 1.92249146e-07, 1.83549745e-03, 3.91037958e-07,\n        3.67221816e-07, 4.31546169e-07, 1.49398588e-07, 1.76727411e-07,\n        5.72315685e-07, 2.30748924e-07, 2.51175010e-07, 9.72664952e-01,\n        2.00999622e-07, 2.84679430e-07, 1.98851055e-07, 3.47812914e-07,\n        2.19516494e-07, 8.47629224e-07],\n       [7.10869297e-13, 3.58482639e-13, 4.13998371e-13, 2.36384743e-08,\n        9.99983191e-01, 1.03281847e-12, 4.10025177e-13, 7.09260396e-13,\n        4.38047439e-13, 8.09454012e-13, 1.39441418e-05, 4.34201069e-13,\n        6.06604665e-13, 5.74433513e-13, 2.54817787e-13, 4.59465745e-13,\n        7.64705248e-13, 2.22449405e-13, 7.80838501e-13, 2.85706642e-06,\n        1.02390947e-12, 4.63325558e-13, 1.82862568e-13, 7.12375905e-13,\n        5.54460395e-13, 6.54676237e-13],\n       [7.46667864e-13, 3.46161549e-13, 3.91953261e-13, 2.36054607e-08,\n        9.99985576e-01, 1.02878632e-12, 4.24269994e-13, 6.96318871e-13,\n        4.32327812e-13, 7.54786695e-13, 1.17981963e-05, 4.28075788e-13,\n        6.20633753e-13, 5.73115069e-13, 2.65788178e-13, 4.45760535e-13,\n        8.02620665e-13, 2.27176825e-13, 7.89387924e-13, 2.59403419e-06,\n        1.05261709e-12, 4.45038429e-13, 1.78731961e-13, 7.16276973e-13,\n        5.80328862e-13, 6.14436019e-13],\n       [2.17532126e-09, 4.35990666e-09, 2.51066568e-09, 9.99764740e-01,\n        1.47460909e-07, 1.32051681e-09, 3.44779960e-09, 3.06217629e-09,\n        6.22373086e-09, 2.82838464e-09, 2.32563092e-04, 4.50781279e-09,\n        7.17544957e-09, 2.76019452e-09, 3.29586736e-09, 1.98488803e-09,\n        1.77098436e-09, 4.81600004e-09, 2.72791767e-09, 2.44046146e-06,\n        1.33191502e-09, 1.01201980e-09, 1.98113237e-09, 3.77013443e-09,\n        2.61857869e-09, 2.12412354e-09],\n       [1.95913463e-09, 4.40383507e-09, 2.33343611e-09, 9.99867678e-01,\n        1.51721039e-07, 1.05106823e-09, 3.23922311e-09, 2.96226199e-09,\n        5.81302873e-09, 2.28725194e-09, 1.30009706e-04, 3.96006827e-09,\n        6.72350531e-09, 2.63612798e-09, 2.69396216e-09, 1.83508431e-09,\n        1.55221114e-09, 5.00093700e-09, 2.43162024e-09, 2.11843781e-06,\n        1.12307041e-09, 8.85532259e-10, 1.68574199e-09, 3.25400418e-09,\n        2.20388285e-09, 1.92464533e-09],\n       [4.13679968e-13, 1.86562273e-13, 2.57826204e-13, 2.03363886e-08,\n        9.99990106e-01, 6.00631253e-13, 2.23343737e-13, 4.12094729e-13,\n        2.18996736e-13, 4.44525466e-13, 8.68089501e-06, 2.25122926e-13,\n        3.41737570e-13, 3.17982647e-13, 1.40027231e-13, 2.49061486e-13,\n        4.13159578e-13, 1.38819918e-13, 4.11609250e-13, 1.23945699e-06,\n        6.15917202e-13, 2.62690341e-13, 1.04967609e-13, 3.81174747e-13,\n        3.19465943e-13, 3.65676917e-13],\n       [4.17399703e-13, 1.82194076e-13, 2.43600848e-13, 1.96697378e-08,\n        9.99990463e-01, 6.08154532e-13, 2.26370816e-13, 4.01639117e-13,\n        2.23934166e-13, 4.17084553e-13, 8.32765636e-06, 2.12248052e-13,\n        3.40157454e-13, 3.13187275e-13, 1.43419863e-13, 2.43363083e-13,\n        4.16952497e-13, 1.36272897e-13, 3.97394709e-13, 1.16115018e-06,\n        6.27150729e-13, 2.64796702e-13, 1.05289482e-13, 3.81205430e-13,\n        3.29829046e-13, 3.46871755e-13],\n       [6.24195096e-08, 1.01068849e-07, 6.18569018e-08, 9.52451944e-01,\n        3.44207074e-04, 4.29185114e-08, 6.84047379e-08, 1.00442733e-07,\n        1.05104597e-07, 6.38686757e-08, 4.62936535e-02, 6.77554723e-08,\n        9.43633367e-08, 6.89207269e-08, 5.12264293e-08, 5.91301905e-08,\n        7.63076571e-08, 8.09358554e-08, 6.12237656e-08, 9.08771763e-04,\n        4.34383374e-08, 4.83799134e-08, 4.75047415e-08, 7.04674505e-08,\n        5.79879433e-08, 9.27268857e-08],\n       [9.12181690e-08, 1.09557888e-07, 8.55770423e-08, 5.46146035e-02,\n        9.74774454e-03, 1.03073340e-07, 8.84041000e-08, 1.35316895e-07,\n        8.50342516e-08, 1.05839156e-07, 9.34857607e-01, 8.49831778e-08,\n        8.60011724e-08, 9.14966236e-08, 7.37116892e-08, 1.10392946e-07,\n        1.27415575e-07, 7.83520164e-08, 1.10648436e-07, 7.77815934e-04,\n        1.12138231e-07, 1.50583077e-07, 7.22216171e-08, 9.51118579e-08,\n        9.11421694e-08, 1.38630526e-07]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# 훈련셋 데이터 예측\n",
    "# 26개 캐릭터의 원핫인코딩 형식\n",
    "results = model.predict(x_train)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 4, 10,  4, 19,  4,  4,  3,  3,  4,  4,  3, 10], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# 1축을 기준으로 최대값의 인덱스 구함\n",
    "results = np.argmax(results, 1) \n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "lov -> love\nloo -> look\nfac -> face\nfas -> fast\nhom -> home\nhop -> hope\ngoo -> good\ngol -> gold\ntre -> tree\ntru -> true\nroa -> road\nroc -> rock\n"
    }
   ],
   "source": [
    "# 예측 결과 출력\n",
    "for i, word in enumerate(word_list):\n",
    "    last_char = char_list[results[i]]\n",
    "    print(word[:3] + ' -> ' + word[:3] + last_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}